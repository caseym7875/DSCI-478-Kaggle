{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Contradictory, My Dear Watson: Multi-Model Classification Write-Up**\n\n---\n\n## 1. Introduction\n\nTextual Entailment (also referred to as Natural Language Inference, or NLI) is a fundamental task in Natural Language Processing (NLP). The goal is to determine whether a *premise* and a *hypothesis* are in a relationship of **entailment**, **contradiction**, or **neutrality**. In the [“Contradictory, My Dear Watson”](https://www.kaggle.com/competitions/contradictory-my-dear-watson) Kaggle competition, we aim to classify pairs of sentences in multiple languages into one of these three categories.\n\nOur approach utilizes **transformer-based** models from the [Hugging Face](https://huggingface.co/) library. Specifically, we experiment with several popular multilingual pretrained models (e.g., **XLM-RoBERTa**, **Multilingual BERT**, and **mDeBERTa**). We then fine-tune them for the classification task, compare their performance, and output predictions for the test data.\n\nBelow, each section corresponds to a notebook cell or set of cells:\n1. **Environment Setup**\n2. **Loading the Dataset**\n3. **Defining Helper Functions**\n4. **Preparing Train/Validation Split**\n5. **Training Multiple Models**\n6. **Comparing & Saving Predictions**\n7. **Compare Early Predictions**\n8. **Conclusion & References**\n\nWe'll also see the core math behind cross-entropy loss, which is minimized during fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"## 2. Problem Description\n\nEach sample in the **Contradictory, My Dear Watson** dataset provides:\n- A **premise** (a sentence).\n- A **hypothesis** (another sentence).\n- A **label** indicating whether the hypothesis is **contradiction**, **entailment**, or **neutral** relative to the premise.\n\nThese examples span **15 different languages**, making it a challenging **multilingual** problem. By leveraging **multilingual pretrained** transformer models, we can capture cross-lingual embeddings to handle these varied inputs.\n\nHence, we treat this as a **3-class** classification task:\n1. **Contradiction**\n2. **Neutral**\n3. **Entailment**","metadata":{}},{"cell_type":"markdown","source":"## 3. Method / Code","metadata":{}},{"cell_type":"markdown","source":"#### Cell 1: Environment Setup\nWe'll install required libraries (`transformers`, etc.) and import the necessary modules.\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport shutil  # For copying best model's CSV\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom tensorflow import keras\n\n# For reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint('Setup complete!')","metadata":{"executionInfo":{}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 2: Loading the Dataset\nWe read the **train** and **test** CSVs. Adjust file paths as necessary for your environment.\n\nFor Kaggle, you might load them from `../input/contradictory-my-dear-watson` or any local path. Here, we just demonstrate reading from the Kaggle or GitHub link.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\n    \"https://raw.githubusercontent.com/caseym7875/DSCI-478-Kaggle/refs/heads/main/train.csv\"\n)\ntest_data = pd.read_csv(\n    \"https://raw.githubusercontent.com/caseym7875/DSCI-478-Kaggle/refs/heads/main/test.csv\"\n)\n\nprint('Train shape:', train_data.shape)\nprint('Test shape:', test_data.shape)\ntrain_data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 3: Defining Helper Functions\nWe define:\n1. **`tokenize_data`**: Loads a tokenizer for a given model and converts premise/hypothesis text to numerical IDs.\n2. **`build_model`**: Creates a Keras model with the pretrained transformer base plus a simple classification head.\n3. **`plot_history`**: Plots training and validation accuracy/loss.\n\nWhen we tokenize, we have inputs truncated or padded to a max length $L$:\n$$\n\\text{encodings} = \\mathrm{Tokenizer}(\\text{premise}, \\text{hypothesis}, \\max\\_length=L).\n$$\n\nIn each training step, we **minimize cross-entropy** for 3 classes:\n$$\n\\mathcal{L} = - \\sum_{i=1}^{3} y_i \\log(\\hat{y}_i),\n$$\nwhere $y$ is the true one-hot label, and $\\hat{y}$ is the predicted probability distribution.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\n\ndef tokenize_data(model_name, dataframe, max_len=128):\n    \"\"\"\n    Tokenize premise/hypothesis using the specified Hugging Face model.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    encodings = tokenizer(\n        list(dataframe['premise'].values),\n        list(dataframe['hypothesis'].values),\n        truncation=True,\n        padding='max_length',\n        max_length=max_len,\n        return_tensors='tf'\n    )\n    return encodings\n\ndef build_model(model_name, max_len=128, num_labels=3, lr=2e-5, dropout_rate=0.3):\n    \"\"\"\n    Create a Keras model using TFAutoModel + a simple classification head.\n    \"\"\"\n    base_model = TFAutoModel.from_pretrained(model_name)\n    input_ids      = keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    \n    outputs = base_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n    cls_token = outputs.last_hidden_state[:, 0, :]  # [CLS]\n    \n    x = keras.layers.Dropout(dropout_rate)(cls_token)\n    x = keras.layers.Dense(num_labels, activation='softmax')(x)\n    \n    model = keras.Model(inputs=[input_ids, attention_mask], outputs=x)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\ndef plot_history(history, title_prefix=\"\"):\n    hist = history.history\n    epochs = range(1, len(hist['loss']) + 1)\n    \n    plt.figure(figsize=(10,4))\n    # Plot Loss\n    plt.subplot(1,2,1)\n    plt.plot(epochs, hist['loss'], 'bo-', label='Train Loss')\n    if 'val_loss' in hist:\n        plt.plot(epochs, hist['val_loss'], 'ro-', label='Val Loss')\n    plt.title(f'{title_prefix} Loss')\n    plt.xlabel('Sub-Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Plot Accuracy\n    plt.subplot(1,2,2)\n    plt.plot(epochs, hist['accuracy'], 'bo-', label='Train Acc')\n    if 'val_accuracy' in hist:\n        plt.plot(epochs, hist['val_accuracy'], 'ro-', label='Val Acc')\n    plt.title(f'{title_prefix} Accuracy')\n    plt.xlabel('Sub-Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Helper functions defined.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 4: Preparing Train/Validation Split\nWe shuffle the training data and split off 10% as a validation set to monitor performance.","metadata":{}},{"cell_type":"code","source":"VALIDATION_SPLIT = 0.1\nval_size = int(len(train_data) * VALIDATION_SPLIT)\n\n# Shuffle\ntrain_data_shuffled = train_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n\nval_data = train_data_shuffled.iloc[:val_size]\ntrain_data_ = train_data_shuffled.iloc[val_size:]\n\ny_val = val_data['label'].values\ny_train = train_data_['label'].values\n\nprint(f\"Training set size: {len(train_data_)}\")\nprint(f\"Validation set size: {len(val_data)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 5: Training Multiple Models\nWe define a list of model candidates:\n1. **XLM-RoBERTa**\n2. **Multilingual BERT**\n3. **mDeBERTa**\n\nFor each:\n1. We **tokenize** train and validation data.\n2. **Build** the Keras model.\n3. **Train** for a specified number of epochs, using cross-entropy.\n4. **Evaluate** on the validation set.\n5. **Predict** on test data.\n6. **Save** each model’s predictions to a CSV.\n\nThe cross-entropy loss is:\n$$\n\\mathcal{L} = -\\sum_{i=1}^3 y_i\\,\\log(\\hat{y}_i),\n$$\nwhere $y_i$ is the one-hot label, and $\\hat{y}_i$ is the predicted probability.","metadata":{}},{"cell_type":"code","source":"model_candidates = [\n    \"joeddav/xlm-roberta-large-xnli\",\n    \"bert-base-multilingual-cased\",\n    \"microsoft/mdeberta-v3-base\"\n]\n\nBATCH_SIZE = 8   # Adjust to fit GPU memory\nEPOCHS = 2       # Full epochs (will be split into sub-epochs)\nMAX_LEN = 64     # Increase if memory allows (e.g., 128/256)\n\nresults = {}                  # model_name -> validation accuracy\npredictions_dict = {}         # model_name -> predicted labels on test\npredictions_csv_dict = {}     # model_name -> CSV path for predictions\nsaved_model_paths = []        # track where each model is saved\n\nfor model_name in model_candidates:\n    print(f\"\\n=== Training model: {model_name} ===\")\n    # Tokenize train & val\n    train_encodings = tokenize_data(model_name, train_data_, max_len=MAX_LEN)\n    val_encodings   = tokenize_data(model_name, val_data,   max_len=MAX_LEN)\n    \n    # Build TF Datasets\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((dict(train_encodings), y_train))\n        .shuffle(buffer_size=len(train_data_))\n        .batch(BATCH_SIZE)\n    )\n    \n    val_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((dict(val_encodings), y_val))\n        .batch(BATCH_SIZE)\n    )\n    \n    # Calculate original steps and then split epochs into sub-epochs\n    original_steps = len(list(train_dataset))\n    subepochs_per_epoch = 10\n    steps_per_subepoch = original_steps // subepochs_per_epoch\n    total_subepochs = EPOCHS * subepochs_per_epoch  # 2 * 10 = 20\n    \n    print(f\"Original steps per epoch: {original_steps}\")\n    print(f\"Sub-epochs per epoch: {subepochs_per_epoch}\")\n    print(f\"Steps per sub-epoch: {steps_per_subepoch}\")\n    print(f\"Total sub-epochs: {total_subepochs}\")\n    \n    # Build model\n    model = build_model(model_name, max_len=MAX_LEN)\n    \n    # Train: Use total_subepochs as the number of epochs and steps_per_subepoch as steps per epoch.\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=total_subepochs,\n        steps_per_epoch=steps_per_subepoch,\n        verbose=1\n    )\n    \n    # Plot training history (20 slices)\n    plot_history(history, title_prefix=model_name)\n    \n    # Evaluate on validation set\n    val_loss, val_acc = model.evaluate(val_dataset, verbose=0)\n    results[model_name] = val_acc\n    print(f\"Validation Accuracy for {model_name}: {val_acc:.4f}\")\n    \n    # Predict on the test set\n    test_encodings = tokenize_data(model_name, test_data, max_len=MAX_LEN)\n    test_dataset = tf.data.Dataset.from_tensor_slices(dict(test_encodings)).batch(BATCH_SIZE)\n    test_preds = model.predict(test_dataset)\n    test_labels = np.argmax(test_preds, axis=1)\n    predictions_dict[model_name] = test_labels\n    \n    # Save model\n    save_path = f\"./{model_name.replace('/', '_')}_model\"\n    model.save(save_path)\n    saved_model_paths.append(save_path)\n    print(f\"Model saved to {save_path}\")\n    \n    # Create and save CSV for predictions\n    pred_csv_path = f\"{model_name.replace('/', '_')}_predictions.csv\"\n    submission_df = pd.DataFrame({\n        'id': test_data['id'],\n        'prediction': test_labels\n    })\n    submission_df.to_csv(pred_csv_path, index=False)\n    predictions_csv_dict[model_name] = pred_csv_path\n    print(f\"Prediction CSV saved as {pred_csv_path}\\n\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 6: Comparing & Saving Predictions\nHere we:\n1. Summarize each model’s validation accuracy.\n2. Pick the **best** model.\n3. Copy that model’s prediction CSV to **`submission.csv`** for Kaggle.\n","metadata":{}},{"cell_type":"code","source":"# Create summary DataFrame\nmodel_performance_df = pd.DataFrame({\n    'Model': list(results.keys()),\n    'Val Accuracy': list(results.values()),\n    'Prediction CSV': [predictions_csv_dict[m] for m in results.keys()]\n}).sort_values(by='Val Accuracy', ascending=False)\n\nprint(\"=== Model Validation Performance ===\")\ndisplay(model_performance_df)\n\n# Select best model\nbest_model_name = model_performance_df.iloc[0]['Model']\nbest_model_csv = model_performance_df.iloc[0]['Prediction CSV']\nprint(f\"\\nBest Model: {best_model_name}\")\nprint(f\"Corresponding CSV: {best_model_csv}\")\n\n# Copy best model's CSV to submission.csv\nshutil.copyfile(best_model_csv, \"submission.csv\")\nprint(\"\\nFinal submission.csv created from the best model!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 7: Compare Early Predictions\nWe can look at the first 10 predictions from each model side by side to see if they disagree on specific samples.\n","metadata":{}},{"cell_type":"code","source":"comparison_df = pd.DataFrame()\ncomparison_df['test_id'] = test_data['id'].head(10)\n\nfor model_name in model_candidates:\n    comparison_df[model_name] = predictions_dict[model_name][:10]\n\ncomparison_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Conclusion\n\nWe have demonstrated a **multilingual classification** approach using pretrained transformers:\n- **XLM-RoBERTa**\n- **Multilingual BERT**\n- **mDeBERTa**\n\nEach was fine-tuned on the **Contradictory, My Dear Watson** dataset for 2 epochs, though **increasing** epochs usually improves final accuracy. We generated test predictions, saved them to separate CSVs, and chose the best model based on validation accuracy.\n\n### Potential Improvements\n- **Longer Training**: More epochs.\n- **Hyperparameter Tuning**: Adjust learning rate, batch size.\n- **Layer Unfreezing**: Fine-tune more layers for potential gains.\n- **Ensembling**: Combine predictions from multiple models.","metadata":{}},{"cell_type":"markdown","source":"\n## 5. References\n- [Vaswani et al., 2017] *Attention Is All You Need.* NIPS.\n- [Devlin et al., 2019] *BERT: Pre-training of Deep Bidirectional Transformers.* NAACL.\n- [Conneau et al., 2020] *Unsupervised Cross-lingual Representation Learning at Scale.* ACL. (XLM-RoBERTa)\n- [He et al., 2021] *DeBERTa: Decoding-Enhanced BERT with Disentangled Attention.* ICLR.\n- [Hugging Face Transformers](https://huggingface.co/docs/transformers)","metadata":{}}]}