{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Contradictory, My Dear Watson: Multi-Model Classification Write-Up**\n\n---\n\n## 1. Introduction\n\nTextual Entailment (also known as Natural Language Inference, or NLI) is a core task in Natural Language Processing (NLP) that involves determining whether a given *hypothesis* can be inferred from a *premise*. Specifically, the relationship is categorized as **entailment**, **contradiction**, or **neutrality**.\n\nIn the [“Contradictory, My Dear Watson”](https://www.kaggle.com/competitions/contradictory-my-dear-watson) Kaggle competition, the challenge is to classify sentence pairs across multiple languages into one of these three classes. Our approach leverages powerful transformer-based models from the [Hugging Face](https://huggingface.co/) library. By fine-tuning several state-of-the-art multilingual models, we aim to determine which architecture best captures the nuances of cross-lingual textual entailment.\n\n> **Notebook Outline:**  \n> 1. Introduction  \n> 2. Problem Description  \n> 3. Model Selection  \n> 4. Method / Code  \n> 5. Conclusion  \n> 6. References\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 2. Problem Description\n\nThe task at hand involves analyzing pairs of sentences, where each pair consists of:\n- A **premise**: a given sentence or statement.\n- A **hypothesis**: another sentence that may or may not be logically inferred from the premise.\n- A **label**: indicating whether the hypothesis is in **entailment**, **contradiction**, or **neutral** relation with the premise.\n\nThis problem is inherently challenging due to its multilingual nature, with data drawn from 15 different languages. The cross-lingual aspect demands models that can robustly capture semantic relationships despite linguistic diversity. By formulating this as a three-class classification problem, we can systematically evaluate the performance of various pretrained transformer models on their ability to generalize across languages.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 3. Model Selection and Tokenization Details\n\nIn our experiments, we compare four prominent multilingual transformer models:\n\n### 1. **XLM-RoBERTa**  \nXLM-RoBERTa, as introduced in [Conneau et al., 2020](https://arxiv.org/abs/1911.02116), is designed for robust cross-lingual performance. \n- XLM-R is a multilingual variant of RoBERTa, trained on **100 languages** using the **CC-100 corpus**.\n- Its architecture and training procedure enable it to capture nuanced semantic relationships across languages, which is critical for tasks like textual entailment.\n\n\n### 2. **Multilingual BERT (mBERT)**  \nMultilingual BERT, detailed in [Devlin et al., 2019](https://arxiv.org/abs/1810.04805), supports 104 languages using a shared WordPiece vocabulary.\n- Trained on the **Wikipedia corpora of 104 languages**, making it highly effective for multilingual tasks.\n- Retains the Next Sentence Prediction (NSP) task, which may be useful for certain textual inference problems.\n- Despite being an earlier model, it remains a **strong baseline** for multilingual NLP.\n\n\n### 3. **mDeBERTa**  \nmDeBERTa is an extension of the DeBERTa architecture as described in [He et al., 2021](https://arxiv.org/abs/2006.03654).\n- Introduces a **disentangled attention mechanism** that separately encodes content and positional embeddings, enhancing contextual learning.\n- Incorporates an **enhanced mask decoder**, helping it outperform standard BERT-based models on classification tasks.\n- Demonstrates strong performance on multilingual tasks due to diverse pretraining.\n\n\n### 4. **mBART**  \nmBART, described in [Liu et al., 2020](https://arxiv.org/abs/2001.08210), is a sequence-to-sequence model pre-trained for multilingual translation tasks.\n- While originally designed for machine translation, its pretrained representations have been successfully fine-tuned for various multilingual NLP tasks, including textual entailment.\n- Its encoder-decoder architecture allows for capturing rich contextual representations across languages.\n\n\n> ### Model Summary\n> All four models restrict inputs to a **512-token limit**, ensuring computational feasibility. However, their tokenization strategies differ slightly:\n> - **XLM-RoBERTa** and **mDeBERTa** both use SentencePiece/BPE tokenization, emphasizing efficiency in subword representation.\n> - **mBERT** relies on WordPiece tokenization, which tends to produce more granular subword units.\n> - **mBART** also uses SentencePiece but is designed as an encoder-decoder model, offering additional flexibility in handling sequence-to-sequence tasks.\n>\n> In terms of overall performance for our multilingual NLI task:\n> - **XLM-RoBERTa** is expected to excel due to its superior cross-lingual representations.\n> - **mBERT** provides a strong baseline, though it is slightly older.\n> - **mDeBERTa** benefits from improved attention mechanisms.\n> - **mBART** offers the potential of leveraging sequence-to-sequence pretraining for enhanced context modeling.\n>\n> Further fine-tuning and hyperparameter optimization tailored to each model could yield additional improvements in classification performance.\n---","metadata":{}},{"cell_type":"markdown","source":"## 4. Method / Code","metadata":{}},{"cell_type":"markdown","source":"#### Cell 1: Environment Setup","metadata":{}},{"cell_type":"code","source":"# !pip install transformers --quiet\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport shutil  # For copying best model's CSV\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom tensorflow import keras\n\n# For reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint('Setup complete!')","metadata":{"executionInfo":{}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 2: Loading the Dataset","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\n    \"https://raw.githubusercontent.com/caseym7875/DSCI-478-Kaggle/refs/heads/main/train.csv\"\n)\ntest_data = pd.read_csv(\n    \"https://raw.githubusercontent.com/caseym7875/DSCI-478-Kaggle/refs/heads/main/test.csv\"\n)\n\nprint('Train shape:', train_data.shape)\nprint('Test shape:', test_data.shape)\ntrain_data.head()\n\n# Calculate average word count for premises and hypotheses\navg_premise_length = train_data['premise'].apply(lambda x: len(x.split())).mean()\navg_hypothesis_length = train_data['hypothesis'].apply(lambda x: len(x.split())).mean()\n\nmax_premise_length = train_data['premise'].apply(lambda x: len(x.split())).max()\nmax_hypothesis_length = train_data['hypothesis'].apply(lambda x: len(x.split())).max()\n\nprint(\"\\nAverage premise length (in words): {:.2f}\".format(avg_premise_length))\nprint(\"Average hypothesis length (in words): {:.2f}\".format(avg_hypothesis_length))\n\nprint(\"\\nMax premise length (in words): {:.2f}\".format(max_premise_length))\nprint(\"Max hypothesis length (in words): {:.2f}\".format(max_hypothesis_length))\n\n# Compute the length (in words) of each premise\npremise_lengths = train_data['premise'].apply(lambda x: len(x.split()))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot the distribution of premise lengths\nplt.figure(figsize=(10,6))\nplt.hist(premise_lengths, bins=30, color='skyblue', edgecolor='black')\nplt.title(\"Distribution of Premise Length (in words)\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The relatively low average word counts in our dataset indicate that using the standard 512-token limit is excessive. To conserve computational resources and reduce processing time, we have set the maximum token length to 128 tokens (defined in next cell).","metadata":{}},{"cell_type":"markdown","source":"#### Cell 3: Defining Helper Functions","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\n\ndef tokenize_data(model_name, dataframe, max_len=128):\n    \"\"\"\n    Loads a tokenizer for a given model and converts premise/hypothesis text to numerical IDs using the specified Hugging Face model.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    encodings = tokenizer(\n        list(dataframe['premise'].values),\n        list(dataframe['hypothesis'].values),\n        truncation=True,\n        padding='max_length',\n        max_length=max_len,\n        return_tensors='tf'\n    )\n    return encodings\n\ndef build_model(model_name, max_len=128, num_labels=3, lr=2e-5, dropout_rate=0.3):\n    \"\"\"\n    Create a Keras model using TFAutoModel + a simple classification head.\n    \"\"\"\n    base_model = TFAutoModel.from_pretrained(model_name)\n    input_ids      = keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    \n    outputs = base_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n    cls_token = outputs.last_hidden_state[:, 0, :]  # [CLS]\n    \n    x = keras.layers.Dropout(dropout_rate)(cls_token)\n    x = keras.layers.Dense(num_labels, activation='softmax')(x)\n    \n    model = keras.Model(inputs=[input_ids, attention_mask], outputs=x)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\ndef plot_history(history, title_prefix=\"\"):\n    \"\"\"\n    Plots training and validation accuracy/loss\n    \"\"\"\n    hist = history.history\n    epochs = range(1, len(hist['loss']) + 1)\n    \n    plt.figure(figsize=(10,4))\n    # Plot Loss\n    plt.subplot(1,2,1)\n    plt.plot(epochs, hist['loss'], 'bo-', label='Train Loss')\n    if 'val_loss' in hist:\n        plt.plot(epochs, hist['val_loss'], 'ro-', label='Val Loss')\n    plt.title(f'{title_prefix} Loss')\n    plt.xlabel('Sub-Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Plot Accuracy\n    plt.subplot(1,2,2)\n    plt.plot(epochs, hist['accuracy'], 'bo-', label='Train Acc')\n    if 'val_accuracy' in hist:\n        plt.plot(epochs, hist['val_accuracy'], 'ro-', label='Val Acc')\n    plt.title(f'{title_prefix} Accuracy')\n    plt.xlabel('Sub-Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Helper functions defined.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 4: Preparing Train/Validation Split","metadata":{}},{"cell_type":"code","source":"\"\"\"\nShuffle the training data and split off 10% as a validation set to monitor performance.\n\"\"\"\n\nVALIDATION_SPLIT = 0.1\nval_size = int(len(train_data) * VALIDATION_SPLIT)\n\ntrain_data_shuffled = train_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n\nval_data = train_data_shuffled.iloc[:val_size]\ntrain_data_ = train_data_shuffled.iloc[val_size:]\n\ny_val = val_data['label'].values\ny_train = train_data_['label'].values\n\nprint(f\"Training set size: {len(train_data_)}\")\nprint(f\"Validation set size: {len(val_data)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 5: Training Multiple Models\nWe define a list of model candidates:\n1. **XLM-RoBERTa**\n2. **mbart-large-cc25**\n3. **Multilingual BERT**\n4. **mDeBERTa**\n\nFor each:\n1. We **tokenize** train and validation data.\n2. **Build** the Keras model.\n3. **Train** for a specified number of epochs, using cross-entropy.\n4. **Evaluate** on the validation set.\n5. **Predict** on test data.\n6. **Save** each model’s predictions to a CSV.","metadata":{}},{"cell_type":"code","source":"model_candidates = [\n    \"joeddav/xlm-roberta-large-xnli\",\n    \"facebook/mbart-large-cc25\",\n    \"bert-base-multilingual-cased\",\n    \"microsoft/mdeberta-v3-base\"\n]\n\n# Training Parameters\nBATCH_SIZE = 8   # Adjust to fit GPU memory\nEPOCHS = 5     # Full epochs (will be split into sub-epochs)\nMAX_LEN = 128     # Increase if memory allows\n\n# Organize Results Data\nresults = {}                  # model_name -> validation accuracy\npredictions_dict = {}         # model_name -> predicted labels on test\npredictions_csv_dict = {}     # model_name -> CSV path for predictions\nsaved_model_paths = []        # track where each model is saved\n\n# Model Training\nfor model_name in model_candidates:\n    print(f\"\\n=== Training model: {model_name} ===\")\n    # Tokenize train & val\n    train_encodings = tokenize_data(model_name, train_data_, max_len=MAX_LEN)\n    val_encodings   = tokenize_data(model_name, val_data,   max_len=MAX_LEN)\n    \n    # Build TF Datasets\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((dict(train_encodings), y_train))\n        .shuffle(buffer_size=len(train_data_))\n        .batch(BATCH_SIZE)\n    )\n    \n    val_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((dict(val_encodings), y_val))\n        .batch(BATCH_SIZE)\n    )\n    \n    # Calculate original steps and then split epochs into sub-epochs\n    original_steps = len(list(train_dataset))\n    subepochs_per_epoch = 10\n    steps_per_subepoch = original_steps // subepochs_per_epoch\n    total_subepochs = EPOCHS * subepochs_per_epoch  # ex) 3 * 10 = 30\n    \n    print(f\"Original steps per epoch: {original_steps}\")\n    print(f\"Sub-epochs per epoch: {subepochs_per_epoch}\")\n    print(f\"Steps per sub-epoch: {steps_per_subepoch}\")\n    print(f\"Total sub-epochs: {total_subepochs}\")\n    \n    # Build model\n    model = build_model(model_name, max_len=MAX_LEN)\n    \n    # Train: Use total_subepochs as the number of epochs and steps_per_subepoch as steps per epoch.\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=total_subepochs,\n        steps_per_epoch=steps_per_subepoch,\n        verbose=1\n    )\n    \n    # Plot training history (30 slices)\n    plot_history(history, title_prefix=model_name)\n    \n    # Evaluate on validation set\n    val_loss, val_acc = model.evaluate(val_dataset, verbose=0)\n    results[model_name] = val_acc\n    print(f\"Validation Accuracy for {model_name}: {val_acc:.4f}\")\n    \n    # Predict on the test set\n    test_encodings = tokenize_data(model_name, test_data, max_len=MAX_LEN)\n    test_dataset = tf.data.Dataset.from_tensor_slices(dict(test_encodings)).batch(BATCH_SIZE)\n    test_preds = model.predict(test_dataset)\n    test_labels = np.argmax(test_preds, axis=1)\n    predictions_dict[model_name] = test_labels\n    \n    # Save model\n    save_path = f\"./{model_name.replace('/', '_')}_model\"\n    model.save(save_path)\n    saved_model_paths.append(save_path)\n    print(f\"Model saved to {save_path}\")\n    \n    # Create and save CSV for predictions\n    pred_csv_path = f\"{model_name.replace('/', '_')}_predictions.csv\"\n    submission_df = pd.DataFrame({\n        'id': test_data['id'],\n        'prediction': test_labels\n    })\n    submission_df.to_csv(pred_csv_path, index=False)\n    predictions_csv_dict[model_name] = pred_csv_path\n    print(f\"Prediction CSV saved as {pred_csv_path}\\n\")","metadata":{"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 6: Comparing & Saving Predictions\nHere we:\n1. Summarize each model’s validation accuracy.\n2. Pick the **best** model.\n3. Copy that model’s prediction CSV to **`submission.csv`** for Kaggle.\n","metadata":{}},{"cell_type":"code","source":"# Create summary DataFrame\nmodel_performance_df = pd.DataFrame({\n    'Model': list(results.keys()),\n    'Val Accuracy': list(results.values()),\n    'Prediction CSV': [predictions_csv_dict[m] for m in results.keys()]\n}).sort_values(by='Val Accuracy', ascending=False)\n\nprint(\"=== Model Validation Performance ===\")\ndisplay(model_performance_df)\n\n# Select best model\nbest_model_name = model_performance_df.iloc[0]['Model']\nbest_model_csv = model_performance_df.iloc[0]['Prediction CSV']\n\n# Copy best model's CSV to submission.csv\nshutil.copyfile(best_model_csv, \"submission.csv\")\n\nprint(f\"\\nBest Model: {best_model_name}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cell 7: Compare Early Predictions\nWe can look at the first 10 predictions from each model side by side to see if they disagree on specific samples.\n","metadata":{}},{"cell_type":"code","source":"comparison_df = pd.DataFrame()\ncomparison_df['test_id'] = test_data['id'].head(10)\n\nfor model_name in model_candidates:\n    comparison_df[model_name] = predictions_dict[model_name][:10]\n\ncomparison_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Conclusion\n\nIn this work, we demonstrated a **multilingual classification** approach using state-of-the-art pretrained transformers, including:\n\n- **XLM-RoBERTa**\n- **Multilingual BERT**\n- **mDeBERTa**\n- **mBART**\n\nEach model was fine-tuned on the **Contradictory, My Dear Watson** dataset for 3 epochs. Although longer training typically improves performance, our experiments showed that even with limited epochs, the models achieved competitive results. Notably, **XLM-RoBERTa** emerged as the best performer. Interestingly, we observed that training XLM-RoBERTa for just 1 epoch resulted in a validation score of 0.89, compared to 0.84 for 2 epochs under the same conditions—likely due to inherent variability in the training process. This suggests that additional training might yield only marginal improvements relative to the extra computational cost.\n\n### Potential Improvements\n- **Longer Training**: Exploring more epochs may help, provided that the validation performance eventually plateaus.\n- **Hyperparameter Tuning**: Fine-tuning the learning rate, batch size, and other parameters could yield further gains.\n- **Layer Unfreezing**: Gradually unfreezing layers during training might allow the model to better adapt to the task.\n- **Data Augmentation**: Incorporating more diverse or augmented training data could help improve robustness.\n\nOverall, the results indicate that pretrained multilingual transformers, particularly XLM-RoBERTa, are highly effective for this complex textual entailment task.","metadata":{}},{"cell_type":"markdown","source":"## 6. References\n\n- **Vaswani et al., 2017**  \n  *Attention Is All You Need.*  \n  In *Advances in Neural Information Processing Systems (NIPS)*.  \n  [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n\n- **Devlin et al., 2019**  \n  *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*  \n  In *North American Chapter of the Association for Computational Linguistics (NAACL)*.  \n  [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)\n\n- **Conneau et al., 2020**  \n  *Unsupervised Cross-lingual Representation Learning at Scale.*  \n  In *Annual Meeting of the Association for Computational Linguistics (ACL)*.  \n  (XLM-RoBERTa)  \n  [https://arxiv.org/abs/1911.02116](https://arxiv.org/abs/1911.02116)\n\n- **He et al., 2021**  \n  *DeBERTa: Decoding-Enhanced BERT with Disentangled Attention.*  \n  In *International Conference on Learning Representations (ICLR)*.  \n  [https://arxiv.org/abs/2006.03654](https://arxiv.org/abs/2006.03654)\n\n- **Liu et al., 2020**  \n  *mBART: Multilingual Denoising Pre-training for Neural Machine Translation.*  \n  arXiv preprint arXiv:2001.08210.  \n  [https://arxiv.org/abs/2001.08210](https://arxiv.org/abs/2001.08210)\n\n- **Hugging Face Transformers**  \n  [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)","metadata":{}}]}